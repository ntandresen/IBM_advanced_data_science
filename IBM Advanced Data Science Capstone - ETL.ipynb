{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('tensorflow': conda)",
   "display_name": "Python 3.7.9 64-bit ('tensorflow': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d344b0b44ed74f5219ec8dab31a9f3e764b5e17a5cf2d9830ceefda51f994047"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# IBM Advanced Data Science Capstone\n",
    "## Extract, Transform, Load\n",
    "ETL is the discipline of transfering data from one or more sources combined into a usable set of data into a destination system. It also involves some preliminary cleaning and data transformation. Mostly, I will focus on collecting nessecary data, combining into one, and preparing for the next step."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pandasql import sqldf\n",
    "import requests\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from shapely.geometry import Point\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet('data/vehicles.parquet.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_capital_lat_long():\n",
    "    # Get state capital data from people.sc.fsu.edu\n",
    "    capitals_ll = requests.get('https://people.sc.fsu.edu/~jburkardt/datasets/states/state_capitals_ll.txt')\n",
    "    # Data is split by new row, value lowered, and converted to dataframe\n",
    "    capitals_ll = capitals_ll.text.split('\\n')\n",
    "    capitals_ll = [c.lower().split() for c in capitals_ll]\n",
    "    capitals_ll = pd.DataFrame(\n",
    "        data=capitals_ll,\n",
    "        columns=['capital', 'lat', 'long']\n",
    "    )\n",
    "    # Set lat and long data type to float\n",
    "    capitals_ll[['lat', 'long']] = capitals_ll[['lat', 'long']].apply(lambda x: x.astype(float))\n",
    "    # Convert lat long to shapely.geometry.Point\n",
    "    geometry = [Point(xy) for xy in zip(capitals_ll['long'], capitals_ll['lat'])]\n",
    "    # Output geopandas dataframe\n",
    "    capitals_ll = gpd.GeoDataFrame(\n",
    "        data=capitals_ll.drop(['lat', 'long'], axis=1),\n",
    "        crs={'init': 'epsg:4326'},\n",
    "        geometry=geometry\n",
    "    )\n",
    "    \n",
    "    return capitals_ll.dropna(subset=['capital'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_neighbors():\n",
    "    capitals_ll = get_capital_lat_long()\n",
    "\n",
    "    # Get list of neighboring states for each state\n",
    "    state_neighbors = requests.get('https://people.sc.fsu.edu/~jburkardt/datasets/states/state_neighbors.txt')\n",
    "    # Perform same transformations as before\n",
    "    state_neighbors = state_neighbors.text.split('\\n')\n",
    "    state_neighbors = [sn.lower().split() for sn in state_neighbors]\n",
    "    max_neighbors = max([len(sn) for sn in state_neighbors])\n",
    "    \n",
    "    states = ['capital']\n",
    "    for i in range(max_neighbors-1):\n",
    "        states.append(f'capital_neighbor_{i}')\n",
    "    \n",
    "    state_neighbors = pd.DataFrame(\n",
    "        data=state_neighbors,\n",
    "        columns=states\n",
    "    )\n",
    "\n",
    "    state_neighbors = state_neighbors \\\n",
    "        .dropna(subset=['capital'])\n",
    "\n",
    "    # Change neighbor states to state capitals lat/long Points\n",
    "    for i, state in enumerate(states):\n",
    "        state_neighbors = state_neighbors \\\n",
    "            .merge(capitals_ll, how='left', left_on=state, right_on='capital') \\\n",
    "            .rename(columns={'geometry': f'cap_{i}'})\n",
    "\n",
    "    keep_cols = [col for col in state_neighbors.columns if col[:7] != 'capital']\n",
    "    \n",
    "    # We still need state abbreviations for main dataset\n",
    "    output = capitals_ll \\\n",
    "        .drop('geometry', axis=1) \\\n",
    "        .merge(state_neighbors[keep_cols], how='left', left_index=True, right_index=True)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(point1, point2):\n",
    "    # Check first if one of the points are blank. If not, continue, else return nan.\n",
    "    if point1 and point2:\n",
    "        # Following formula from https://en.wikipedia.org/wiki/Haversine_formula\n",
    "        lon1, lat1, lon2, lat2 = map(np.radians, [point1.x, point1.y, point2.x, point2.y])\n",
    "        dlon = lon2 - lon1\n",
    "        dlat = lat2 - lat1\n",
    "\n",
    "        a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "        c = 2*np.arcsin(np.sqrt(a))\n",
    "        km = 6371*c\n",
    "        return km\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert main data lat and long to Points and append to data.\n",
    "# Hereafter, the output from get_state_neighbors() is joined onto the main data.\n",
    "geometry = [Point(xy) for xy in zip(data['long'], data['lat'])]\n",
    "g_data = gpd.GeoDataFrame(\n",
    "    data=data,\n",
    "    crs={'init': 'epsg:4326'},\n",
    "    geometry=geometry\n",
    ").merge(\n",
    "    get_state_neighbors(),\n",
    "    how='left',\n",
    "    left_on='state',\n",
    "    right_on='capital'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating haversine distances between vehicle state and (neighboring) capital(s).\n",
    "capitals = [col for col in g_data.columns.values if (col[:3] == 'cap') & (col != 'capital')]\n",
    "for i, capital in enumerate(capitals):\n",
    "    g_data[f'distance_{i}'] = g_data.apply(lambda x: haversine_distance(x['geometry'], x[capital]), axis=1)\n",
    "    g_data = g_data.drop(capital, axis=1)"
   ]
  },
  {
   "source": [
    "g_data.describe()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "               price           year      odometer            lat  \\\ncount  265866.000000  265855.000000  2.261230e+05  264114.000000   \nmean    15669.512687    2010.307596  9.916456e+04      38.826539   \nstd     11917.514833       9.171975  1.081257e+05       5.945867   \nmin      2000.000000    1911.000000  0.000000e+00     -80.386400   \n25%      6975.000000    2007.000000  4.621050e+04      35.119200   \n50%     12500.000000    2012.000000  9.300000e+04      39.462150   \n75%     20495.000000    2016.000000  1.374955e+05      42.598200   \nmax     78900.000000    2021.000000  9.999999e+06      80.383400   \n\n               long     distance_0     distance_1     distance_2  \\\ncount  264114.00000  264114.000000  259629.000000  257845.000000   \nmean      -93.80658     249.848497     495.211429     505.351872   \nstd        17.77061     364.156530     353.449924     364.040311   \nmin      -161.39400       0.015682       0.154056       1.181180   \n25%      -106.49800      73.378253     291.682487     268.202785   \n50%       -87.90650     168.890080     427.663599     464.152595   \n75%       -81.00790     301.114559     629.837336     637.688634   \nmax       115.52400   15903.621465   15893.018748   15563.655551   \n\n          distance_3     distance_4     distance_5    distance_6  \\\ncount  225516.000000  171338.000000  101656.000000  59050.000000   \nmean      549.721586     542.868669     495.771869    553.028821   \nstd       423.605099     381.153040     376.088126    341.395884   \nmin         1.497300       0.095287       0.546068      0.830410   \n25%       285.286762     285.672330     262.684336    366.928001   \n50%       460.345733     440.507785     403.915371    501.461818   \n75%       714.240088     722.553522     638.306018    681.103694   \nmax     15610.385172   15352.239521   11951.400180  12518.862660   \n\n         distance_7    distance_8  \ncount  14122.000000  10694.000000  \nmean     555.333489    564.272050  \nstd      287.790485    206.546242  \nmin       35.914750     29.146037  \n25%      338.462465    464.446351  \n50%      558.526287    544.622575  \n75%      702.232763    689.066750  \nmax    12197.275975   6701.692403  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>price</th>\n      <th>year</th>\n      <th>odometer</th>\n      <th>lat</th>\n      <th>long</th>\n      <th>distance_0</th>\n      <th>distance_1</th>\n      <th>distance_2</th>\n      <th>distance_3</th>\n      <th>distance_4</th>\n      <th>distance_5</th>\n      <th>distance_6</th>\n      <th>distance_7</th>\n      <th>distance_8</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>265866.000000</td>\n      <td>265855.000000</td>\n      <td>2.261230e+05</td>\n      <td>264114.000000</td>\n      <td>264114.00000</td>\n      <td>264114.000000</td>\n      <td>259629.000000</td>\n      <td>257845.000000</td>\n      <td>225516.000000</td>\n      <td>171338.000000</td>\n      <td>101656.000000</td>\n      <td>59050.000000</td>\n      <td>14122.000000</td>\n      <td>10694.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>15669.512687</td>\n      <td>2010.307596</td>\n      <td>9.916456e+04</td>\n      <td>38.826539</td>\n      <td>-93.80658</td>\n      <td>249.848497</td>\n      <td>495.211429</td>\n      <td>505.351872</td>\n      <td>549.721586</td>\n      <td>542.868669</td>\n      <td>495.771869</td>\n      <td>553.028821</td>\n      <td>555.333489</td>\n      <td>564.272050</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>11917.514833</td>\n      <td>9.171975</td>\n      <td>1.081257e+05</td>\n      <td>5.945867</td>\n      <td>17.77061</td>\n      <td>364.156530</td>\n      <td>353.449924</td>\n      <td>364.040311</td>\n      <td>423.605099</td>\n      <td>381.153040</td>\n      <td>376.088126</td>\n      <td>341.395884</td>\n      <td>287.790485</td>\n      <td>206.546242</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>2000.000000</td>\n      <td>1911.000000</td>\n      <td>0.000000e+00</td>\n      <td>-80.386400</td>\n      <td>-161.39400</td>\n      <td>0.015682</td>\n      <td>0.154056</td>\n      <td>1.181180</td>\n      <td>1.497300</td>\n      <td>0.095287</td>\n      <td>0.546068</td>\n      <td>0.830410</td>\n      <td>35.914750</td>\n      <td>29.146037</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>6975.000000</td>\n      <td>2007.000000</td>\n      <td>4.621050e+04</td>\n      <td>35.119200</td>\n      <td>-106.49800</td>\n      <td>73.378253</td>\n      <td>291.682487</td>\n      <td>268.202785</td>\n      <td>285.286762</td>\n      <td>285.672330</td>\n      <td>262.684336</td>\n      <td>366.928001</td>\n      <td>338.462465</td>\n      <td>464.446351</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>12500.000000</td>\n      <td>2012.000000</td>\n      <td>9.300000e+04</td>\n      <td>39.462150</td>\n      <td>-87.90650</td>\n      <td>168.890080</td>\n      <td>427.663599</td>\n      <td>464.152595</td>\n      <td>460.345733</td>\n      <td>440.507785</td>\n      <td>403.915371</td>\n      <td>501.461818</td>\n      <td>558.526287</td>\n      <td>544.622575</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>20495.000000</td>\n      <td>2016.000000</td>\n      <td>1.374955e+05</td>\n      <td>42.598200</td>\n      <td>-81.00790</td>\n      <td>301.114559</td>\n      <td>629.837336</td>\n      <td>637.688634</td>\n      <td>714.240088</td>\n      <td>722.553522</td>\n      <td>638.306018</td>\n      <td>681.103694</td>\n      <td>702.232763</td>\n      <td>689.066750</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>78900.000000</td>\n      <td>2021.000000</td>\n      <td>9.999999e+06</td>\n      <td>80.383400</td>\n      <td>115.52400</td>\n      <td>15903.621465</td>\n      <td>15893.018748</td>\n      <td>15563.655551</td>\n      <td>15610.385172</td>\n      <td>15352.239521</td>\n      <td>11951.400180</td>\n      <td>12518.862660</td>\n      <td>12197.275975</td>\n      <td>6701.692403</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ]
  },
  {
   "source": [
    "We'll create an extra feature from this, which is going to be the average distance from the data latitude and longitude to the top three nearest state capitals."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_state_distance():\n",
    "    state_neighbors = get_capital_lat_long()\n",
    "    n = len(state_neighbors[['capital', 'geometry']])\n",
    "    output = []\n",
    "    for i in range(n):\n",
    "        state = state_neighbors['capital'][i]\n",
    "        capital = state_neighbors['geometry'][i]\n",
    "        distances = []\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                neighbor = state_neighbors['geometry'][j]\n",
    "                distances.append(haversine_distance(capital, neighbor))\n",
    "        output.append([state, np.mean(sorted(distances)[:3])])\n",
    "    return pd.DataFrame(output, columns=['state', 'distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_data = g_data.merge(\n",
    "    get_average_state_distance(),\n",
    "    how='left',\n",
    "    left_on='state',\n",
    "    right_on='state'\n",
    ").drop('geometry', axis=1)"
   ]
  },
  {
   "source": [
    "Before we move on to further external features, let's also do something about a few other, so far, categorical features. Namely `cylinders`, `condition`, `size`, and `title_status`. The `cylinders` column can already be considered as continuous, where higher value is \"better\". We'll simply remove the text 'cylinders' and convert to integer:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_data['cylinders'] = pd.to_numeric(g_data['cylinders'].str.replace(r'[^0-9]', '').str.strip())"
   ]
  },
  {
   "source": [
    "### Additional External Features\n",
    "Let's find some more information about our cars. Luckily, the British market research site YouGov provides some exciting information, ranking car brands by popularity and fame, grouped by eg. sex and age group. We'll be focusing solely on an overall rating. Using [Selenium](https://selenium-python.readthedocs.io/) in combination with [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) we can scrape the site for information. We need to make the website think it's accessed by an actual browser. We'll use Google Chrome for this:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perusing yougov.com, we expect at least fifty entries. \n",
    "# This will keep running, until this condition is satisfied\n",
    "rows = 0\n",
    "while rows < 50:\n",
    "    CHROME_PATH = 'C:\\Program Files%s\\Google\\Chrome\\Application\\chrome.exe'\n",
    "    CHROMEDRIVER_PATH = 'chromedriver\\chromedriver.exe'\n",
    "\n",
    "    # Set some options (don't open the actual browser)\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')\n",
    "\n",
    "    try:\n",
    "        chrome_options.binary_location = CHROME_PATH % ''\n",
    "        driver = webdriver.Chrome(\n",
    "            executable_path=CHROMEDRIVER_PATH,\n",
    "            chrome_options=chrome_options\n",
    "        )\n",
    "    except:\n",
    "        chrome_options.binary_location = CHROME_PATH % ' (x86)'\n",
    "        driver = webdriver.Chrome(\n",
    "            executable_path=CHROMEDRIVER_PATH,\n",
    "            chrome_options=chrome_options\n",
    "        )\n",
    "    # Get popularity\n",
    "    driver.get('https://today.yougov.com/ratings/automotive/popularity/car-makers/all')\n",
    "    for _ in range(20):\n",
    "        # All rankings only becomes visible if we scroll down\n",
    "        driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "        sleep(0.1)\n",
    "    soup = BeautifulSoup(driver.page_source)\n",
    "\n",
    "    ratings = []\n",
    "    # Search for html tags in the soup and save to array\n",
    "    for s in soup.find_all('li', attrs={'class': 'ng-star-inserted'}):\n",
    "        if s.find('h2') is not None:\n",
    "            brand = s.find('h2').contents[0].strip().lower()\n",
    "            for x in s.find_all('span', attrs={'_yg-server-content-sc67': ''}):\n",
    "                if len(x.contents) > 1:\n",
    "                    rating = x.contents[0].strip()\n",
    "            ratings.append([brand, float(rating)])\n",
    "\n",
    "    # Get fame\n",
    "    driver.get('https://today.yougov.com/ratings/automotive/fame/car-makers/all')\n",
    "    for _ in range(20):\n",
    "        driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "        sleep(0.1)\n",
    "    soup = BeautifulSoup(driver.page_source)\n",
    "\n",
    "    fames = []\n",
    "    # Search for html tags in the soup and save to array\n",
    "    for s in soup.find_all('li', attrs={'class': 'ng-star-inserted'}):\n",
    "        if s.find('h2', attrs={'class': 'title'}) is not None:\n",
    "            brand = s.find('h2', attrs={'class': 'title'}).contents[0].strip().lower()\n",
    "            for x in s.find_all('span', attrs={'_ngcontent-yg-app-c53': ''}):\n",
    "                if len(x.contents) > 1:\n",
    "                    fame = float(x.contents[0].strip())\n",
    "            fames.append([brand, fame])\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    # Combine the two results into a dataframe\n",
    "    popularity = pd.DataFrame(ratings, columns=['manufacturer', 'popularity']) \\\n",
    "        .set_index('manufacturer') \\\n",
    "        .merge(\n",
    "            pd.DataFrame(fames, columns=['manufacturer', 'fame']) \\\n",
    "                .set_index('manufacturer'),\n",
    "            how='outer',\n",
    "            left_index=True,\n",
    "            right_index=True\n",
    "        ) \\\n",
    "        .reset_index()\n",
    "\n",
    "    rows = popularity.dropna().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(57, 3)"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "popularity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the popularity factors onto the main data\n",
    "g_data = g_data.merge(\n",
    "    popularity,\n",
    "    how='left',\n",
    "    left_on='manufacturer',\n",
    "    right_on='manufacturer'\n",
    ")"
   ]
  },
  {
   "source": [
    "The last columns we can infer additional information from are the `vin` and `description` columns. VIN (Vehicle Idintification Number) is a unique identifier every vehicle has, and from this information about make, model, engine size, equipment, etc. can be obtained. There is a wealth of online VIN decoders out there, some free but most requires a paid subscription. Fortunately the [NHTSA](https://www.nhtsa.gov/) (National Highway Traffic Saftety Administration) offers up a free API to decode VIN's in batches. Unfortunately it takes a lot of time (roughly 4 hours for this amount of data), so I have provided the code to obtain decoded VIN's below:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vin_decoder(data):\n",
    "    # Link to api for batch lookups\n",
    "    URI = 'https://vpic.nhtsa.dot.gov'\n",
    "    endpoint = f'/api/vehicles/DecodeVINValuesBatch/?format=json'\n",
    "    # Inject VINs a json \n",
    "    data = {'data': ';'.join(data)}\n",
    "    # Get response and convert to dataframe\n",
    "    r = requests.post(URI + endpoint, data=data)\n",
    "    j = json.loads(r.content)\n",
    "    # Remove redundant columns\n",
    "    df = pd.DataFrame(j['Results']) \\\n",
    "        .replace(r'(^\\s*$)|(Not Applicable)', np.nan, regex=True) \\\n",
    "        .dropna(how='all', axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique VINs\n",
    "vins = g_data['vin'].dropna().unique()\n",
    "\n",
    "# Wrap VINs into batches of 50 and lookup on NHTSA\n",
    "vin_df = pd.DataFrame()\n",
    "for i in range(0, len(vins_missing), 50):\n",
    "    vin_df = vin_df.append(vin_decoder(data=vins_missing[i:i+50])\n",
    "\n",
    "# Checkpoint: Save the data\n",
    "vin_df.to_parquet('data/vin_decoded.parquet.gzip', compression='gzip'))"
   ]
  },
  {
   "source": [
    "Decoded VINs checkpoint!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vin_df = pd.read_parquet('data/vin_decoded.parquet.gzip')"
   ]
  },
  {
   "source": [
    "The information obtained from NHTSA yields some interesting additional information. Let's first sort through the observations and only consider the columns with less than 50% missing values:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "EngineKW               0.476979\nEngineHP               0.476911\nEngineModel            0.474641\nEngineConfiguration    0.380934\nGVWR                   0.368322\nTPMS                   0.312859\nSeries                 0.309635\nDriveType              0.309544\nPlantState             0.286194\nPlantCompanyName       0.280484\nAirBagLocSide          0.228233\nDoors                  0.197789\nPlantCity              0.173667\nSeatBeltsAll           0.135310\nAirBagLocFront         0.133721\nEngineCylinders        0.121949\nPlantCountry           0.075090\nFuelTypePrimary        0.071163\nDisplacementCC         0.037608\nDisplacementCI         0.037608\nDisplacementL          0.037608\nBodyClass              0.028833\nModelID                0.028243\nModel                  0.028243\nModelYear              0.025382\nManufacturer           0.024894\nVehicleType            0.024894\nManufacturerId         0.024894\nMakeID                 0.024894\nMake                   0.024894\nVIN                    0.016153\nErrorText              0.016153\nErrorCode              0.016153\ndtype: float64"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "vin_df_nans = vin_df.isna().sum().sort_values(ascending=False)/vin_df.shape[0]\n",
    "vin_df_nans[vin_df_nans < 0.5]"
   ]
  },
  {
   "source": [
    "From the above, going by rule of thumb, the features that are interesting are the following:\n",
    "\n",
    "| Column | | Description | | Comments |\n",
    "| :- | --- | :- | --- | :- |\n",
    "| `EngineKW`/`EngineHP` | | Engine power in kW and horsepower | | This column has a high percentage of missings, but hopefully some information can be gained regardless, eg. by averaging over make and model |\n",
    "| `EngineConfiguration` || Categorisation of the engines |||\n",
    "| `GVWR` || Gross vehicle weight rating |||\n",
    "| `TPMS` || Tire-pressure monitoring system |||\n",
    "| `Doors` || Number of doors || Can be used as additional information on the size of the vehicle |\n",
    "| `EngineCylinders` || Number of cylinders in engine || Should be the same as `cylinders` in the main data. We'll take it in case the main data is missing. |\n",
    "| `DisplacementXX` || Engine size |||\n",
    "| `ModelYear` || Model year || Should be the same as `year` in the main data, but we'll grab it for good measure |\n",
    "| `VIN` || Vehicle identification number || Neccesary to combine this data with the main data |\n",
    "\n",
    "Before merging dataframes, let's convert the columns into correct data types. `EngineKW`/`EngineHP`, `Doors`, `EngineCylinders`, `ModelYear`, and the `DisplacementXX` columns should be numerical, while the rest are categorical."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_vin_df(df):\n",
    "    num_cols = [\n",
    "        'enginekw',\n",
    "        'enginehp',\n",
    "        'doors',\n",
    "        'enginecylinders',\n",
    "        'modelyear',\n",
    "        'displacementcc',\n",
    "        'displacementci',\n",
    "        'displacementl'\n",
    "    ]\n",
    "    cat_cols = [\n",
    "        'gvwr',\n",
    "        'tpms',\n",
    "        'engineconfiguration',\n",
    "        'manufacturer',\n",
    "        'make',\n",
    "        'model',\n",
    "        'vin'\n",
    "    ]\n",
    "\n",
    "    df.columns = [x.lower() for x in df.columns]\n",
    "    df[num_cols] = df[num_cols].astype(float)\n",
    "    df[cat_cols[:-1]] = df[cat_cols[:-1]].apply(lambda x: x.str.lower())\n",
    "    df['gvwr'] = df['gvwr'].str.replace(r'(\\:.*)', '')\n",
    "    df = df[cat_cols + num_cols]\n",
    "    df['vin2'] = df['vin'].str[3:10]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vin_df = preprocess_vin_df(vin_df)"
   ]
  },
  {
   "source": [
    "As one last, final thing, we notice that the `model` column is very messy, eg. Ford F-150 can be listed multiple times with various extra text, which we'll assume is information on the model variant. The model of a vehicle is very important when determining the sales price. It has a big say wether it is a Ford Fiesta or a Ford Mustang, so keeping this feature is crucial. In order to simplify to the model level, we need to somehow fix this. One way would be to manually map every single unique model entry by hand. However, as we see below, there are over 20,000 unique entries in `model` (which if left untreated would essentially leave us with ~20,000 dummy variables, which adds too much complexity to the model), and it would therefore be tedious work. Another way is by being a bit clever. Not accounting for spelling errors, we should be safe to assume that a \"mazda3 sport sedan 4d\", which is in the Craigslist dataset, can be mapped simply to \"mazda3\", which we obtained from the VIN previously. The solution I propose is to loop over every model in every brand, and check if the VIN model is in the Craigslist model by comparing strings. If we can map the two, we keep it. Else, we set it to NaN."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "f-150               5081\nsilverado 1500      3338\nescape              2738\n1500                2453\ncamry               2357\n                    ... \ntahoe lt custom        1\naccord lx wagon        1\nf-150 4wd lariat       1\nsupercab 4x4           1\ns-10 blazer suv        1\nName: model, Length: 19027, dtype: int64"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "g_data['model'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dictionary for mapping\n",
    "model_map = {}\n",
    "\n",
    "# Find all unique brands in vin_df and loop\n",
    "brands = vin_df['make'].unique()\n",
    "brands = [str(b).lower() for b in brands if b is not None]\n",
    "for brand in brands:\n",
    "    # Set the value for the current brand as an empty dictionary\n",
    "    model_map[brand] = {}\n",
    "\n",
    "    # Find all unique models for currrent brand in vin_df\n",
    "    models_vin = vin_df.loc[vin_df['make'].str.lower() == brand, 'model'].unique()\n",
    "    models_vin = [str(m).lower() for m in models_vin if m is not None]\n",
    "    # Find all unique models for currrent brand in Craigslist dataset\n",
    "    models = g_data.loc[g_data['manufacturer'] == brand, 'model'].unique()\n",
    "\n",
    "    # Loop over models in model_vin and compare\n",
    "    for model in models_vin:\n",
    "        _model = re.sub(r'[^0-9a-z]', '', model)\n",
    "        for m in models:\n",
    "            if re.sub(r'[^0-9a-z]', '', model) in re.sub(r'[^0-9a-z]', '', m):\n",
    "                model_map[brand][m] = model\n",
    "\n",
    "    # Map the dictionary on the model column for current brand\n",
    "    g_data.loc[g_data['manufacturer'] == brand, 'model'] = g_data \\\n",
    "        .loc[g_data['manufacturer'] == brand, 'model'] \\\n",
    "        .map(model_map[brand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "f-150                    10084\nsilverado                 9752\nf-250                     4967\n1500                      4792\nsierra                    4436\n                         ...  \nv-rod                        1\nlr2 awd                      1\ndavidson 72                  1\npanemera 4                   1\nrange sport autobiogr        1\nName: model, Length: 885, dtype: int64"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "g_data['model'].value_counts()"
   ]
  },
  {
   "source": [
    "We reduced the number of unique models from 19,000 to just about 900. That's effective data cleaning!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "According to [Wikipedia](https://en.wikipedia.org/wiki/Vehicle_identification_number#Components), digits 4 to 8(9) of the Vehicle Identification Number contains vehicle descripter features such as make, model, engine configuration, displacement, and so on. We'll create a new column with only those digits, which we'll use to help combine the newly obtained VIN information with the original data. Since not all vehicles in our original dataset have an associated VIN, we need to enrich these rows in some other way. Below is an SQL query, that calculates the averages of the numerical features of `vin_df` and ranks them according to the grouping parameters:\n",
    "\n",
    "| Rank || Condition |\n",
    "| :- | --- | :- |\n",
    "| 1 || Direct match on `vin` |\n",
    "| 2 || If no previous match, match on `vin2` |\n",
    "| 3 || If no previous match, match on `make`, `model` and `year` |\n",
    "| 4 || If no previous match, match on `make` and `model` |\n",
    "| 5 || If no previous match, match on `make` |\n",
    "\n",
    "Rank 1 and 2 are the best options. Rank 3 is good, but since the displacement and engine size most likely varies with variant and vehicle type, this introduces higher uncertainty that the match is correct. This will be the case as we lessen the constraints on the join parameters through ranks 4 and 5."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_data['vin2'] = g_data['vin'].str[3:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pysqldf = lambda q: sqldf(q, globals())\n",
    "g_data = pysqldf(\"\"\"\n",
    "    select\n",
    "        g_data.*\n",
    "      , t1.gvwr\n",
    "      , t1.tpms\n",
    "      , t1.engineconfiguration\n",
    "      , case\n",
    "            when length(g_data.model) <= length(coalesce(t1.model, t3.model, t4.model, g_data.model))\n",
    "            then g_data.model\n",
    "            else coalesce(t1.model, t3.model, t4.model, g_data.model)\n",
    "        end as model_new\n",
    "      , coalesce(t1.displacementcc, t2.displacementcc, t3.displacementcc, t4.displacementcc, t5.displacementcc) as displacementcc\n",
    "      , coalesce(t1.displacementci, t2.displacementci, t3.displacementci, t4.displacementci, t5.displacementci) as displacementci\n",
    "      , coalesce(t1.displacementl, t2.displacementl, t3.displacementl, t4.displacementl, t5.displacementl) as displacementl\n",
    "      , coalesce(t1.enginekw, t2.enginekw, t3.enginekw, t4.enginekw, t5.enginekw) as enginekw\n",
    "      , coalesce(t1.enginehp, t2.enginehp, t3.enginehp, t4.enginehp, t5.enginehp) as enginehp\n",
    "      , coalesce(t1.enginecylinders, t2.enginecylinders, t3.enginecylinders, t4.enginecylinders, t5.enginecylinders) as enginecylinders\n",
    "      , round(coalesce(t1.doors, t2.doors, t3.doors, t4.doors, t5.doors), 0) as doors\n",
    "      , coalesce(t1.vin_avg_weight, t2.vin_avg_weight, t3.vin_avg_weight, t4.vin_avg_weight, t5.vin_avg_weight) as vin_avg_weight\n",
    "    from g_data\n",
    "\tleft join (\n",
    "\t\tselect\n",
    "\t\t\t*\n",
    "\t\t  , 1 as vin_avg_weight\n",
    "\t\tfrom vin_df\n",
    "\t) t1\n",
    "        on g_data.vin = t1.vin\n",
    "\tleft join (\n",
    "        select\n",
    "            vin2\n",
    "          , avg(displacementcc) as displacementcc\n",
    "          , avg(displacementci) as displacementci\n",
    "          , avg(displacementl) as displacementl\n",
    "          , avg(enginekw) as enginekw\n",
    "          , avg(enginehp) as enginehp\n",
    "          , avg(enginecylinders) as enginecylinders\n",
    "          , avg(doors) as doors\n",
    "          , 2 as vin_avg_weight\n",
    "        from vin_df\n",
    "        where make is not null\n",
    "        group by \n",
    "            make\n",
    "    ) t2\n",
    "        on g_data.vin2 = t2.vin2\n",
    "    left join (\n",
    "        select\n",
    "            make\n",
    "          , model\n",
    "          , modelyear\n",
    "          , avg(displacementcc) as displacementcc\n",
    "          , avg(displacementci) as displacementci\n",
    "          , avg(displacementl) as displacementl\n",
    "          , avg(enginekw) as enginekw\n",
    "          , avg(enginehp) as enginehp\n",
    "          , avg(enginecylinders) as enginecylinders\n",
    "          , avg(doors) as doors\n",
    "          , 3 as vin_avg_weight\n",
    "        from vin_df\n",
    "        where make is not null\n",
    "        group by \n",
    "            make\n",
    "          , model\n",
    "          , modelyear\n",
    "    ) t3\n",
    "        on g_data.manufacturer = t3.make\n",
    "        and g_data.model = t3.model\n",
    "        and g_data.year = t3.modelyear\n",
    "    left join (\n",
    "        select\n",
    "            make\n",
    "          , model\n",
    "          , avg(displacementcc) as displacementcc\n",
    "          , avg(displacementci) as displacementci\n",
    "          , avg(displacementl) as displacementl\n",
    "          , avg(enginekw) as enginekw\n",
    "          , avg(enginehp) as enginehp\n",
    "          , avg(enginecylinders) as enginecylinders\n",
    "          , avg(doors) as doors\n",
    "          , 2 as vin_avg_weight\n",
    "        from vin_df\n",
    "        where make is not null\n",
    "        group by \n",
    "            make\n",
    "          , model\n",
    "    ) t4\n",
    "        on g_data.manufacturer = t4.make\n",
    "        and g_data.model = t4.model\n",
    "    left join (\n",
    "        select\n",
    "            make\n",
    "          , avg(displacementcc) as displacementcc\n",
    "          , avg(displacementci) as displacementci\n",
    "          , avg(displacementl) as displacementl\n",
    "          , avg(enginekw) as enginekw\n",
    "          , avg(enginehp) as enginehp\n",
    "          , avg(enginecylinders) as enginecylinders\n",
    "          , avg(doors) as doors\n",
    "          , 1 as vin_avg_weight\n",
    "        from vin_df\n",
    "        where make is not null\n",
    "        group by \n",
    "            make\n",
    "    ) t5\n",
    "        on g_data.manufacturer = t5.make\n",
    "    \n",
    ";\n",
    "\"\"\")\n",
    "g_data = g_data \\\n",
    "  .drop(['model', 'region', 'vin', 'vin2'], axis=1) \\\n",
    "  .rename(columns={'model_new': 'model'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_data.to_parquet('data/vehicles_etl.parquet.gzip', compression='gzip')"
   ]
  }
 ]
}